---
layout: post
---

_Historia de la informática_

No hay palabra de la que se haya abusado más al hablar de informática que «revolución». Si creemos lo que dicen la prensa diaria y la televisión, cada modelo nuevo de chip, cada componente nuevo de software, cada nuevo adelanto en las redes sociales y cada modelo nuevo de teléfono móvil u otro dispositivo portátil cambiarán nuestra vida de forma revolucionaria. Unas semanas más tarde el objeto de esos reportajes curiosamente queda olvidado y pasa a sustituirse por un nuevo avance, el cual, se nos asegura, constituye, esta vez sí, el verdadero punto de inflexión.

Sin embargo es indiscutible que el efecto de la tecnología informática en la vida diaria del ciudadano de a pie ha sido revolucionario. Sólo con medir la capacidad de cálculo de estas máquinas, tomando como referencia la cantidad de datos que pueden almacenar y recuperar de su memoria interna, se pone de manifiesto un ritmo de progreso que ninguna otra tecnología, ni antigua ni moderna, ha alcanzado. No hace falta recurrir a los lenguajes especializados de ingenieros o programadores informáticos, pues la enorme cantidad de ordenadores y aparatos digitales que hay instalados en nuestros hogares y oficinas o que los consumidores llevan de un lado a otro por todo el mundo revela un ritmo de crecimiento parecido y que no da muestras de estar aminorando. Una medida aún más significativa nos la proporciona lo que estas máquinas son capaces de hacer. El transporte aéreo comercial, la recaudación de impuestos, la administración e investigación médica, la planificación y las operaciones militares; estas y muchísimas otras actividades llevan el sello indeleble del apoyo informático, sin el cual serían muy diferentes o, sencillamente, no existirían.

Al intentar resumir la historia de la informática a lo largo de las últimas décadas nos enfrentamos a la dificultad de escribir en medio de esta fulgurante evolución. Si queremos hacerlo con el rigor debido, habremos de reconocer que tiene sus raíces históricas en la base de la civilización, que en parte se ha caracterizado por la capacidad de las personas de manejar y almacenar información por medio de símbolos. Pero en ella también debemos recoger los rápidos avances y la difusión vertiginosa de que ha sido objeto desde 1945, lo que no es fácil, si queremos conservar simultáneamente la perspectiva histórica. Este artículo es un breve repaso de las personas, las máquinas, las instituciones y los conceptos fundamentales que constituyen la revolución informática tal y como la conocemos en la actualidad. Empieza con el ábaco —que además del primero por orden alfabético es, cronológicamente, uno de los primeros instrumentos de cálculo— y llega hasta el siglo xxi, en el que las redes de ordenadores personales se han convertido en algo habitual y en el que la potencia informática ha terminado por integrarse en minúsculos dispositivos portátiles.

Aunque los aparatos digitales siguen evolucionando a mayor velocidad que nunca, los ordenadores personales se han estancado. Sus componentes físicos se han estabilizado: un teclado (procedente de la famosa máquina de escribir de la década de 1890); una caja rectangular que contiene los circuitos electrónicos y la unidad de almacenamiento, y encima de ella, un terminal de visualización (heredero de la ya mítica pantalla de televisión de finales de la década de 1940). Lo mismo ha ocurrido con los circuitos electrónicos que hay en su interior, al margen de que cada año tengan mayor capacidad: durante los últimos treinta y cinco años han estado compuestos de circuitos integrados de silicio revestidos de tubos de plástico negro montados en paneles también de plástico. Los ordenadores portátiles dieron al traste con esta configuración, pero esencialmente son iguales. Tanto ingenieros como usuarios están de acuerdo en que su diseño físico presenta numerosos inconvenientes. Pensemos, por ejemplo, en las lesiones de los músculos de las manos que se producen por el uso excesivo de un teclado que se diseñó hace un siglo. Ahora bien, todavía no ha tenido éxito ninguno de los muchos intentos por lograr una potencia, una versatilidad y una facilidad de uso equivalentes en otras plataformas, en especial en teléfonos portátiles.

Los programas que estos ordenadores ejecutan, el software, continúan evolucionando a gran velocidad, como también lo hacen los elementos a los que están conectados, las bases de datos y las redes mundiales de comunicaciones. Es imposible prever adónde nos llevará todo ello. En el lapso de tiempo que transcurrirá desde la redacción de este ensayo hasta su publicación, puede que la naturaleza de la informática haya cambiado tanto que algunas partes de este estudio habrán quedado obsoletas. Los ingenieros de Silicon Valley hablan de que los avances en informática se desarrollan en tiempo Internet, unos seis años más rápido de lo que lo hacen en cualquier otro lugar. Incluso tras eliminar parte de esta hipérbole publicitaria, esta observación parece ser cierta.

Los orígenes de la informática pueden situarse al menos en cuatro momentos históricos. El primero es el más obvio: la Antigüedad, cuando civilizaciones nacientes empezaron a ayudarse de objetos para calcular y contar tales como las piedrecillas (en latín calculi, del que viene el término actual calcular), los tableros de cálculo y los ábacos, todos los cuales han llegado hasta el siglo xx (Aspray 1990).

Ahora bien, ninguno de estos instrumentos se parece a lo que hoy nos referimos con el término ordenador. Para los ciudadanos de la época actual, un ordenador es un dispositivo o conjunto de dispositivos que nos libera de la pesadez que suponen las tareas de cálculo, así como de la actividad paralela de almacenar y recuperar información. Por tanto, el segundo hito histórico en la historia de la informática sería 1890, año en el que Herman Hollerith concibió la tarjeta perforada junto con un sistema de máquinas que procesaban, evaluaban y clasificaban la información codificada en ellas para la elaboración del censo de Estados Unidos. El sistema de Hollerith surgió en un momento crucial de la historia: cuando la maquinaria mecánica, cuyo mayor exponente son el motor de vapor y las turbinas hidráulicas y de vapor, había transformado la industria. La conexión entre energía y producción hacía necesaria una mayor supervisión, no sólo física, también de la gestión de datos que la industrialización trajo consigo. Los tabuladores de Hollerith (y la empresa que éste fundó y que sería la base del grupo IBM) fueron una de tantas respuestas, pero hubo otras, como las máquinas eléctricas de contabilidad, las cajas registradoras, las máquinas de sumar mecánicas, la conmutación automática y los mecanismos de control para los ferrocarriles, las centrales telefónicas y telegráficas junto con los sistemas de información para los mercados internacionales de valores y materias primas.

No obstante, el lector actual podría quejarse y aducir que éste tampoco es el punto de partida adecuado. Parece que la auténtica revolución informática guarda relación con la electrónica, si no con los microprocesadores de silicio, que en la actualidad están en todas partes, al menos con sus antepasados inmediatos, los transistores y los tubos de vacío. Según esto, la era de la informática comenzó en febrero de 1946, cuando el ejército de Estados Unidos hizo público el Calculador e integrador numérico electrónico (Electronic Numerical Integrator and Computer, ENIAC) en un acto celebrado en la Moore School of Electrical Engineering de Filadelfia. El ENIAC, que contaba con 18.000 tubos de vacío, se presentó como un instrumento capaz de calcular la trayectoria de un proyectil lanzado desde un cañón antes de que el proyectil realizara el recorrido. Eligieron muy bien el ejemplo, pues este tipo de cálculos era el motivo por el cual el ejército había invertido más de medio millón de dólares de entonces (lo que equivaldría a varios millones de dólares en la actualidad) en una técnica que, se reconocía, era arriesgada y estaba por demostrar.

Un estudio histórico reciente ha desvelado que previamente existía otra máquina que realizaba operaciones de cálculo con tubos de vacío. Se trata del Colossus británico, del que se fabricaron varias unidades que se instalaron en Bletchley Park, Inglaterra, durante la Segunda Guerra Mundial, y se usaron con éxito para descifrar los códigos alemanes. A diferencia del ENIAC, estas máquinas no realizaban operaciones aritméticas convencionales, pero sí llevaban a cabo operaciones de lógica a gran velocidad, y al menos algunas de ellas llevaban varios años en funcionamiento antes de la presentación pública del invento estadounidense. Tanto el ENIAC como el Colossus estuvieron precedidos de un dispositivo experimental que diseñó en la Universidad de Iowa un catedrático de Física llamado John V. Atanasoff, con la colaboración de Clifford Berry. Esta máquina también realizaba operaciones de cálculo por medio de tubos de vacío, pero, aunque sus componentes principales se presentaron en 1942, nunca llegó a estar en funcionamiento (Burks y Burks 1988).

El lector podría observar de nuevo que lo fundamental no es simplemente que una tecnología exista, sino que pase a ser de uso habitual en las mesas de trabajo y los hogares del ciudadano normal. Después de todo no han sido muchas las personas, como máximo una docena, que hayan tenido la oportunidad de utilizar el ENIAC y sacar provecho de su extraordinaria potencia. Lo mismo ocurre con los ordenadores Colossus, que se desmontaron después de la Segunda Guerra Mundial. Según esto, habría que fechar el verdadero origen de la revolución informática no en 1946 sino en 1977, año en el que dos jóvenes, Steve Jobs y Steve Wozniak, originarios de lo que se conoce como Silicon Valley, dieron a conocer al mundo un ordenador llamado Apple II. El Apple II (al igual que su predecesor inmediato el Altair y su sucesor el IBM PC) sacó a la informática del mundo especializado de las grandes empresas y el ejército y la llevó al resto del mundo.

Podríamos seguir indefinidamente con este debate. Según los jóvenes de hoy, la revolución informática es aún más reciente, pues consideran que se produjo cuando, gracias a Internet, un ordenador en un lugar determinado intercambió información con ordenadores que estaban en otros lugares. La más famosa de estas redes la creó la Agencia de proyectos de investigación avanzada (Advance Research Projects Agency, ARPA) del Departamento de Defensa de Estados Unidos, que a principios de 1969 ya tenía una red en marcha (ARPANET). Sin embargo, también hubo otras redes que conectaron ordenadores personales y miniordenadores. Cuando éstas se combinaron, en la década de 1980, nació Internet tal y como hoy la conocemos (Abbate 1999).

Lo cierto es que hay muchos puntos donde se puede empezar esta historia. Mientras escribo este artículo la informática está experimentando una nueva transformación. Me refiero a la fusión entre ordenadores personales y dispositivos de comunicación portátiles. Como en otras ocasiones, esta transformación viene acompañada de descripciones en la prensa diaria que hablan de los efectos revolucionarios que tendrá. Es evidente que el teléfono posee una historia larga e interesante, pero no es ése el tema que nos ocupa. Sólo hay una cosa clara: aún no hemos asistido al último episodio de este fenómeno. Habrá muchos más cambios en el futuro, todos impredecibles, todos presentados como el último adelanto de la revolución informática y todos dejarán relegadas al olvido las «revoluciones» anteriores.

Este relato comienza a principios de la década de 1940. La transición de los ordenadores mecánicos a los electrónicos fue, en efecto, importante, pues entonces se sentaron las bases para inventos posteriores, como los ordenadores personales. En aquellos años ocurrieron más cosas importantes: fue durante esta década cuando surgió el concepto de programación (posteriormente ampliado al de software) como actividad independiente del diseño de los equipos informáticos, si bien de suma importancia para que éstos pudieran emplearse para lo que habían sido diseñados. Por último, fue en esta época cuando, como resultado de la experiencia con las primeras enormes computadoras experimentales ya en funcionamiento, apareció un diseño funcional básico, una arquitectura, para utilizar el término más reciente, que se ha mantenido a través de las oleadas sucesivas de avances tecnológicos hasta la actualidad.

Por tanto, y con todos los matices que habrá que añadir para que la afirmación resulte admisible para los historiadores, podemos considerar que el ENIAC constituyó el eje de la revolución informática (Stern 1981). Aquella máquina, concebida y desarrollada en la Universidad de Pensilvania durante la Segunda Guerra Mundial, inauguró lo que conocemos por era informática. Siempre y cuando se entienda que cualquier punto de origen histórico que se elija es en cierto modo arbitrario, y siempre y cuando se conceda el debido crédito a los adelantos que tuvieron lugar antes, incluida la labor de Babbage y Hollerith, así como los inventos de la máquina de sumar, la caja registradora y otros dispositivos similares, podemos empezar aquí.

